---
title: "p8105_hw5_co2616"
output: github_document
date: "2025-11-14"
---

loading key packages
```{r}
library(tidyverse)
library(httr)
```

## Problem 1
 
creating a function where, for a fixed group size, randomly draws ‚Äúbirthdays‚Äù for each person; checks whether there are duplicate birthdays in the group; and returns TRUE or FALSE based on the result.
```{r}
bday_sim = function (n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)

  repeated_bday = length(unique(birthdays)) < n_room

  repeated_bday
}
```

running this function 10000 times for each group size between 2 and 50 and computing the probability that at least two people in the group will share a birthday

```{r}
bday_sim_results = 
  expand_grid(
    group_size = 2:50,
    iter = 1:1000
  ) %>% 
  mutate(
    results = map_lgl(group_size, bday_sim)
  ) %>% 
  group_by(
    group_size
  ) %>% 
  summarise(
    prob_repeat = mean(results)
  )
```

plotting results

```{r}
bday_sim_results %>% 
  ggplot(aes(x = group_size, y = prob_repeat)) +
  geom_point() +
  geom_line()

```

As number of people in a room increases, there is a greater probability that at least two people share a birthday. In a room full of about 22 people, there would be around a 50% chance that at least two people share a birthday and in a room full of about 50 people, there would be about a 99% chance that at least two people share a birthday. 

## Problem 2

```{r}
null_df = function(n, mu = 0, sigma = 5) {
  
 sim_df =
  tibble(
    x = rnorm(n = n, mean = mu, sd = sigma)
    )
}

```

```{r}
sim_results_df = 
  expand_grid(
    sample_size = 30,
    iter = 1:5000
  ) %>%  
  mutate(
    sample = map(sample_size, null_df),
    test = map(sample, t.test),
    tidy = map(test, broom::tidy)
  ) %>%  
  unnest(tidy) %>% 
  select(iter, sample_size, estimate, p.value)
```

**Repeating the above for ùúá={1,2,3,4,5,6}**

```{r}
new_null_df = function(n, mu, sigma = 5) {
  
 new_sim_df =
  tibble(
    x = rnorm(n = n, mean = mu, sd = sigma)
    )
}
```

```{r}
new_sim_results_df = 
  expand_grid(
    sample_size = 30,
    mu_list = c(1,2,3,4,5,6),
    iter = 1:5000
  ) %>%  
  mutate(
    new_sample = map2(sample_size, mu_list, new_null_df),
    new_test = map(new_sample, t.test),
    new_tidy = map(new_test, broom::tidy)
  ) %>%  
  unnest(new_tidy) %>% 
  select(iter, sample_size, mu_list, estimate, p.value)
```

```{r}
new_sim_results_df %>%
  group_by(mu_list) %>%
  summarize(
    power = mean(p.value < 0.05)
  ) %>% 
  ggplot(aes(x = mu_list, y = power)) +
  geom_line()
```
As effect size increases, the power increases and when the true mean is 4, you are reaching max power. 

A plot showing the average estimate of ùúáÃÇ on the y axis and the true value of ùúá on the x axis.
```{r}
new_sim_results_df %>%
  group_by(mu_list) %>%
  summarize(
    avg_estimate = mean(estimate)
  ) %>% 
  ggplot(aes(x = mu_list, y = avg_estimate)) +
  geom_line()
```

A plot showing the average estimate of ùúáÃÇ only in samples for which the null was rejected on the y axis and the true value of ùúá on the x axis.
```{r}
new_sim_results_df %>%
  group_by(mu_list) %>%
  filter(p.value < 0.05) %>% 
  summarize(
    avg_estimate = mean(estimate)
  ) %>% 
  ggplot(aes(x = mu_list, y = avg_estimate)) +
  geom_line()
```
Overall no. However, as the effect size increases, the sample average of ùúáÃÇ across tests for which the null is rejected approximately is equal to the true value of ùúá because power increases.

## Problem 3

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("NA", ".", "")) %>% 
  janitor::clean_names() %>% 
  unite("city_state", city, state, sep = ", " ) %>% 
  group_by(city_state) %>% 
  summarise(
    total_homicides = n(),
    unsol_homicides = sum(disposition %in% c("Close without arrest", "Open/No arrest"))
    )
```
The `homicide_df` dataset has `r nrow(homicide_df)` observations and `r ncol(homicide_df)` variables and it shows the number of homicides across 50 large U.S. cities. This dataset contains key variables such as `victim_age` which shows the age of the victim of the homicide, `city` which shows the city in which the homicide occurred, and `dispostion` which tells us the outcome of the homicide. 

**Baltimore, MD**

Running a `prop.test` for Baltimore, MD in my dataset and extracting both the proportion of unsolved homicides and the confidence interval.

```{r}
baltimore = 
  homicide_df %>%
  filter(city_state == "Baltimore, MD")

baltimore_test = 
  prop.test(baltimore$unsol_homicides, baltimore$total_homicides)

baltimore_test %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```

**All Cities**

Setting up a function to run a `prop.test` for each of the cities in my dataset.

```{r}
city_test = function(x, n) {
  
  prop.test(x = x, n = n)

}

```

Running a `prop.test` for each of the cities in my dataset and extracting both the proportion of unsolved homicides and the confidence interval for each.

```{r}
homicide_df_results = 
  homicide_df %>% 
  mutate(
    data = map2(unsol_homicides, total_homicides, city_test),
    results = map(data, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  select(city_state, estimate, conf.low, conf.high)

homicide_df_results
```

plotting results

```{r}
homicide_df_results %>%
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "steelblue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 1))
  
```

